{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jameschoong7/weekly/blob/master/Image_Processing_Study_ListCAI3044N_Study_List_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Processing Study List - Based on Lecture Notes\n",
        "\n",
        "Here is a study list based on the provided lecture notes, including the topics and details as presented in the document.\n",
        "\n",
        "**Theory**\n",
        "\n",
        "* **Applications of image processing**\n",
        "    * Image enhancement: Recover or improve visual quality. [cite: 110]\n",
        "    * Feature extraction: Extract colours and shapes to find region of interest (ROI). [cite: 111]\n",
        "    * Object recognition: Training images of an object for recognition. [cite: 112]\n",
        "    * Image retrieval: Search and shortlisting relevant images in big data. [cite: 113]\n",
        "    * Steganography: Embed and detect messages or watermarks in digital images for security purposes. [cite: 113]\n",
        "    * Image compression: Reduce redundancy in image data to save memory space. [cite: 114]\n",
        "\n",
        "* **Computer graphics vs computer vision**\n",
        "    * Computer Graphics: Forward engineering, Define image/video content, Produce graphics elements using a graphics accelerator, Convert graphics elements into frames as final render, Simulate motion and animation using transformation matrices. [cite: 47, 48, 49, 50] Render images and videos. [cite: 51]\n",
        "    * Computer Vision: Reverse engineering, Interpret image/video content, Retrieve visual data from either image file format, digital video file format or live digital camera, Write visual data into either digital images, video, live camera, Detect motions in videos. [cite: 47, 48, 49, 50] Interpret images and videos content. [cite: 51]\n",
        "\n",
        "* **Raster image vs Vector image**\n",
        "    * Raster image: An image is usually reads pixel by pixel, from left to right, then line by line. [cite: 71] Image can be read from either one of the corners, which are top left, top right, bottom left, and bottom right. [cite: 72] E.g. RAW start read at top left, while TIFF start at bottom left. [cite: 73] A raster image generates pixelated problem even with high resolution at certain levels of zooming. [cite: 79]\n",
        "    * Vector image: It uses mathematical computation to form an image. [cite: 74] Feature points are interpolated into outlines and fill each region with shades. [cite: 75] It is usually refers to scalable vector graphics (SVG). [cite: 76] SVG currently still in developments by World Wide Web Consortium (W3C). [cite: 77] In fact, SVG is a programming language by itself. [cite: 78] A vector image generates smooth curves given any scaling factors. [cite: 80]\n",
        "\n",
        "* **Human Vision vs Machine Vision**\n",
        "    * Human Visual System (HVS): Eyes Perceived vision, V1 Spatial information, V2, V3, and V4 Extraction, segmentation, and classification, V5 Recognition. [cite: 148]\n",
        "    * Machine Vision: Camera/ frame grabber, Image representation, Hierarchical clustering, Regression analysis. [cite: 148]\n",
        "\n",
        "* **5 Stages of Image Processing System**\n",
        "    * Pre-processing: It is mainly to remove clutter in the image, i.e. noise reduction and contrast stretching, etc. [cite: 178]\n",
        "    * Feature extraction: A descriptor is used to interpret an aspect of an image content, i.e. colour, edges, shapes, textures, etc. All features are then collected and saved in a vector, called feature vector. [cite: 178]\n",
        "    * Feature selection: Pick relevant features from feature vector to find region of interest (ROI). [cite: 179]\n",
        "    * Classification or image segmentation: Partition an image into meaningful regions, called super-pixels. [cite: 180]\n",
        "    * Post-processing: It is mainly to rectify miss classify pixels. [cite: 180]\n",
        "\n",
        "* **Image compression & file format - uncompressed vs. lossy vs. loseless.**\n",
        "    * Image compression: To reduce the physical size of an image, there are two approaches of image compression, which are lossy compression and lossless compression. [cite: 236]\n",
        "    * Uncompressed file formats: RAW image file format[cite: 214], Tagged image file format (TIFF or TIF)[cite: 220], BMP (bitmap)[cite: 226].\n",
        "    * Lossy compression: Lossy compression, whereby certain details of data have been statistically smoothened or removed. [cite: 237] The process is usually irreversible, where original information will be lost. [cite: 238] E.g. lossy compression techniques include Gaussian noise elimination and discrete cosine transform (DCT). [cite: 239]\n",
        "    * Lossless compression: Lossless compression, whereby details of data are stored completely. [cite: 240] The process is usually reversible, where original data can be either retrieved or recovered. [cite: 241] E.g. lossless compression techniques include Huffman coding, Lempel-Ziv-Welch (LZW), and discrete wavelet transform (DWT). [cite: 242]\n",
        "\n",
        "* **Image representation - Greyscale vs Colour image**\n",
        "    * Greyscale image: It is a single channel or monochrome image. [cite: 247] It has a two dimensional matrix with resolution defines as width times height. [cite: 248] The aspect ratio is defining as width: height. [cite: 249] For a 8 bits colour depth in a channel, it has 2$^8$ or 256 levels of intensities. [cite: 249]\n",
        "    * Colour image: It is a three or four channels chromatic image, (e.g. BGR, RGB, or RGBA). [cite: 250] It has two dimensional matrix with resolution defines as width times height. [cite: 251] The aspect ratio is defining as width: height. [cite: 252] For a 8 bits colour depth in a channel, it has 2$^8$ or 256 levels of intensities. [cite: 252] Noted that each channel is stored in a byte. [cite: 253]\n",
        "\n",
        "* **Image watermarking**\n",
        "    * Steganography: Embed and detect messages or watermarks in digital images for security purposes. [cite: 113]\n",
        "\n",
        "* **Perceptual hashing**\n",
        "    * Information on perceptual hashing is not available in the provided lecture notes.\n",
        "\n",
        "* **Legal & ethical issues**\n",
        "    * Legal and ethical issues in image processing system is a lesson topic. [cite: 29, 228]\n",
        "\n",
        "**Computation/Calculation/Algorithm**\n",
        "\n",
        "* **MSE & PSNR**\n",
        "    * Mean squared error (MSE) [cite: 184]\n",
        "        $$\n",
        "        MSE = \\frac{1}{XY} \\sum_{y=0}^{y-1} \\sum_{x=0}^{x-1} (i_{ori} - i_{seg})^2\n",
        "        $$ [cite: 184, 760]\n",
        "        where $i_{ori}$ is the intensity for a pixel in original image, and $i_{seg}$ is the intensity for a pixel in segmented image. [cite: 760]\n",
        "    * Peak signal-to-noise ratio (PSNR), d = maximum possible value for a pixel [cite: 184]\n",
        "        For sample with N bits per pixel, $d = 2^N - 1$. [cite: 184]\n",
        "        $$\n",
        "        PSNR = 20 \\log_{10} \\left( \\frac{d}{\\sqrt{MSE}} \\right)\n",
        "        $$ [cite: 184]\n",
        "        or\n",
        "        $$\n",
        "        PSNR = 20 \\log_{10} \\left( \\frac{d-1}{MSE} \\right)\n",
        "        $$ [cite: 184]\n",
        "        or\n",
        "        $$\n",
        "        PSNR = 20 \\log \\frac{d - 1}{MSE}\n",
        "        $$ [cite: 760]\n",
        "\n",
        "* **Confusion matrix**\n",
        "    * Human versus machine\n",
        "        |                 | Machine declares positive | Machine declares negative |\n",
        "        | :-------------- | :------------------------ | :------------------------ |\n",
        "        | Human declares positive | True positive (TP)        | False negative (FN)       |\n",
        "        | Human declares negative | False positive (FP)       | True negative (TN)        | [cite: 185]\n",
        "    * Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$ [cite: 185]\n",
        "\n",
        "* **Compression ratio**\n",
        "    * Compression ratio is the size of compressed image, ci over the size of original image, oi. [cite: 261]\n",
        "    * Compression ratio, cr = size of compressed image, ci / size of original image, oi. [cite: 262]\n",
        "    * [ size of original image also means the size of the uncompressed image ]. [cite: 263]\n",
        "\n",
        "* **Image convolution**\n",
        "    * It is a filtering process between an image with a spatial mask. [cite: 287]\n",
        "    * A spatial mask is placed on top of the neigbourhood pixels for an image. [cite: 288]\n",
        "    * Then, it computes the linear combination of the neigbourhood of pixels in the image and structuring elements in the spatial mask. [cite: 289]\n",
        "    * The result is the updated value for each pixel, p. [cite: 290]\n",
        "\n",
        "* **Image segmentation - thresholding**\n",
        "    * Image segmentation: It splits an image into a finite number of partitions. [cite: 273] Ideally, each partition should be a meaningful region that human can understand. [cite: 274] It is an initial process for image classification and object recognition. [cite: 275]\n",
        "    * Global Thresholding: To split an image into two distinct classes, one can define a threshold value $T$, such that $B(x,y) = 1$ if $I(x,y) \\ge T$, otherwise $0$. [cite: 579] $B(x,y)$ is a Boolean value for a given pixel location $x,y$, $I(x,y)$ is an intensity for a given pixel in an image, and $T$ is a threshold value. [cite: 579] Effective to apply on images which has bright objects on dark background, as well as on dark objects on bright background. [cite: 580]\n",
        "    * Local Thresholding: Global thresholding could fail if lighting environment for an image background is uneven. [cite: 582] It partitions the image into a grid of blocks. [cite: 583] Each block has its own localize threshold value, $T(x,y)$. [cite: 584]\n",
        "\n",
        "* **Image segmentation - k-means**\n",
        "    * K-means clustering algorithm: It requires the image data points and k number of clusters (or classes). [cite: 595] It classifies each data point into a respective cluster. [cite: 596] For each data point, assign it to a cluster with the nearest centroid. [cite: 597] Move the centroid to the new mean (or also known as the centre of gravity) for each cluster. [cite: 598] Repeat the assignment and centroid update until all centroids converged i.e. when no more re-assignment of data points to different cluster. [cite: 599] Note that k-means clustering algorithm is simple and reasonably fast. [cite: 600]\n",
        "\n",
        "* **Morphological operations**\n",
        "    * Morphological operations: It either increases or decreases the boundary for an image. [cite: 308]\n",
        "    * Erosion: It uses hit-or-miss operation. [cite: 405] When all ones in the neighbourhood pixels will be one, otherwise zero. [cite: 405]\n",
        "    * Dilation: It requires multiple hit-and-miss operations. [cite: 407] When neighbourhood pixels are matched then the current pixel will be set to one, otherwise the value will be kept. [cite: 407]\n",
        "\n",
        "**Filters that you need to know**\n",
        "\n",
        "* **Low-pass filters - mean, median, Gaussian, bilateral filters**\n",
        "    * Low-pass filtering or image blurring: It detects the region of interest (ROI) for an image (i.e. smooth surface, homogeneous region, background, etc.), while reduces the edges. [cite: 306] It refers to image blurring. [cite: 336] It detects the region of interest (ROI) for an image to gain homogeneous regions. [cite: 336] It de-noise an image signal. [cite: 337] Over use could reduce the contrast between ROIs. [cite: 337]\n",
        "    * Averaging filter (Mean filter): It is the most fundamental mask. [cite: 339] The structuring elements for this mask are of value equal to one. [cite: 340] It computes by taking the average over the neigbourhood pixels with a given 2D size of mask. [cite: 341]\n",
        "    * Gaussian blur: Sometimes it refers to weighted average mask. [cite: 344] The Gaussian filter is a non-uniform low pass filter. [cite: 352] The kernel coefficients diminish with increasing distance from the kernel’s centre. [cite: 353] Centre pixels have a higher weightage than those further away from the centre. [cite: 354] Larger values of $\\sigma$ produce a wider peak (i.e. greater blurring). [cite: 355] Kernel size must increase with increasing $\\sigma$ to maintain the Gaussian nature of the filter. [cite: 356] Gaussian kernel coefficients depend on the value of $\\sigma$. [cite: 357] At the edge of the mask, coefficients must be close to 0. [cite: 358] The kernel is rotationally symmetric with no directional bias. [cite: 358] Gaussian kernel is separable which allows fast computation. [cite: 359]\n",
        "    * Median blur: [cite: 324, 338, 156] (No detailed explanation of the technique in the provided text)\n",
        "    * Bilateral filter: [cite: 361] (No detailed explanation of the technique in the provided text)\n",
        "\n",
        "* **High-pass filters - Prewitt, Sobel, Laplacian, Canny filters**\n",
        "    * High-pass filtering or edge detection: It detects the edges/texture, while reduces the content of an image. [cite: 307] It refers to 3 × 3 zero crossing operator. [cite: 366] It computes the differences in the horizontal, vertical, and diagonal directions. [cite: 367] The end result for the convolution is the gradient coefficients. [cite: 368] It can employs a thresholding technique to achieve edge detections. [cite: 369]\n",
        "    * Prewitt operator: It is the fundamental mask to produce gradient coefficients for an image. [cite: 372] Prewitt masks are illustrated. [cite: 373]\n",
        "    * Sobel operator: It is the mask to produce gradient coefficients with smoothening effects for an image. [cite: 375] Sobel masks are illustrated. [cite: 376]\n",
        "    * Laplacian: [cite: 325, 165, 378, 380] (No detailed explanation of the technique in the provided text)\n",
        "    * Canny filters: [cite: 325, 165, 378, 380] (No detailed explanation of the technique in the provided text)\n",
        "\n",
        "* **Morphological transformations**\n",
        "    * Morphological operations: It either increases or decreases the boundary for an image. [cite: 308]\n",
        "    * Erosion: It uses hit-or-miss operation. [cite: 405] When all ones in the neighbourhood pixels will be one, otherwise zero. [cite: 405] cv.erode(src, kernel, iterations = 1) [cite: 419]\n",
        "    * Dilation: It requires multiple hit-and-miss operations. [cite: 407] When neighbourhood pixels are matched then the current pixel will be set to one, otherwise the value will be kept. [cite: 407] cv.dilate(src, kernel, iterations = 1) [cite: 419]\n",
        "    * Opening: Erosion followed by dilation to de-blob. [cite: 410] cv.morphologyEx(src, cv.MORPH_OPEN, kernel) [cite: 419]\n",
        "    * Closing: Dilation followed by erosion. [cite: 410] cv.morphologyEx(src, cv.MORPH_CLOSE, kernel) [cite: 419]\n",
        "    * Morphological gradient: Get difference between dilation and erosion of an image. [cite: 411] This could generate an outline. [cite: 411] dilation - erosion [cite: 420]\n",
        "    * Top hat: Difference between input image and opening of the image. [cite: 412] src - opening [cite: 420]\n",
        "    * Black hat: Difference between input image and closing of the image. [cite: 413] closing - src [cite: 421]\n",
        "\n",
        "**Able to write OpenCV statements, or functions for:**\n",
        "\n",
        "* **LPF & HPF filters**\n",
        "    * Image convolution with averaging filter:"
      ],
      "metadata": {
        "id": "iEc12kfN018W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = np.ones((5,5), np.float32)/(5*5)\n",
        "        smoothed = cv2.filter2D(img, -1, kernel)\n",
        "        ``` [cite: 323]\n",
        "    * Averaging filter (Mean filter):\n",
        "        ```python\n",
        "        kernel = np.ones((15,15), np.float32)/225\n",
        "        smoothed = cv2.filter2D(res, -1, kernel)\n",
        "        ``` [cite: 338]\n",
        "    * Gaussian blur:\n",
        "        ```python\n",
        "        blur = cv2.GaussianBlur(res, (15,15), 0)\n",
        "        ``` [cite: 338]\n",
        "    * Median blur:\n",
        "        ```python\n",
        "        median = cv2.medianBlur(res, 15)\n",
        "        ``` [cite: 338]\n",
        "    * Sobel filters:\n",
        "        ```python\n",
        "        sobelx = cv2.Sobel(frame, cv2.CV_64F, 1, 0, ksize=5)\n",
        "        sobely = cv2.Sobel(frame, cv2.CV_64F, 0, 1, ksize=5)\n",
        "        ``` [cite: 325, 378]\n",
        "    * Laplacian:\n",
        "        ```python\n",
        "        laplacian = cv2.Laplacian(frame, cv2.CV_64F)\n",
        "        ``` [cite: 325, 378]\n",
        "    * Canny filters:\n",
        "        ```python\n",
        "        edges = cv2.Canny(frame, 100, 100)\n",
        "        ``` [cite: 325, 378]\n",
        "\n",
        "* **RGB & Hue colour classifications**\n",
        "    * Split a colour image to its individual Red, Green and Blue planes:\n",
        "        * `cv2.split()` and `cv2.merge()` [cite: 13]\n",
        "        * array splicing [cite: 14]\n",
        "        * Numpy array manipulation functions. [cite: 14]\n",
        "    * Classify the image to R, G and B segments: Find the dominant primary colour (R or G or B) for each pixel, and classify based on dominant colour. [cite: 16, 17]\n",
        "    * Colour space conversion in Python OpenCV:\n",
        "        * RGB to Grayscale: `gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)` or `gray = cv2.imread(img, cv2.IMREAD_GRAYSCALE)` [cite: 365]\n",
        "        * RGB to YUV: `YUV = cv.cvtColor(img, cv.COLOR_BGR2YUV)` [cite: 365]\n",
        "        * RGB to YCbCr: `YCC = cv.cvtColor(img, cv.COLOR_BGR2YCrCb)` [cite: 365]\n",
        "        * RGB to HSV: `HSV = cv.cvtColor(img, cv.COLOR_BGR2HSV)` [cite: 365]\n",
        "        * RGB to CIELab: `LAB = cv.cvtColor(img, cv.COLOR_BGR2Lab)` [cite: 365]\n",
        "\n",
        "* **Gamma correction**\n",
        "    * Gamma correction is a light adjustments toward an input image. [cite: 439] It is a non-linear operation which is used to encode and decode luminance values in greyscale and RGB images. [cite: 440] It is defined as a power-law expression, $V_{out} = cV_{in}^\\gamma$, where $V_{out}$ is the output voltage, $V_{in}$ is the input voltage, $c$ is the constant (usually $c = 1$), and $\\gamma$ is the gamma value. [cite: 441] (No specific OpenCV function for gamma correction calculation provided in the text, but the concept and effects are discussed).\n",
        "\n",
        "* **DFT, DCT and their applications**\n",
        "    * Discrete Fourier Transform (DFT): [cite: 509] It counts the regularity of periods with given a fixed wavelength, $\\lambda$. [cite: 509] Its formula consists of cosine and sine functions. [cite: 510] Suitable to use in noise reduction and detect texture regions. [cite: 519]\n",
        "    * Discrete Cosine Transform (DCT): [cite: 528] Its formula consisted of cosine functions. [cite: 528] (No specific OpenCV functions for DFT or DCT directly provided in the text, but their concepts and domains are discussed).\n",
        "\n",
        "* **Morphological operations**\n",
        "    * Erosion: `cv.erode(src, kernel, iterations = 1)` [cite: 419]\n",
        "    * Dilation: `cv.dilate(src, kernel, iterations = 1)` [cite: 419]\n",
        "    * Opening: `cv.morphologyEx(src, cv.MORPH_OPEN, kernel)` [cite: 419]\n",
        "    * Closing: `cv.morphologyEx(src, cv.MORPH_CLOSE, kernel)` [cite: 419]\n",
        "    * Morphological gradient: `dilation - erosion` [cite: 420]\n",
        "    * Top hat: `src - opening` [cite: 420]\n",
        "    * Black hat: `closing - src` [cite: 421]\n",
        "\n",
        "* **Corner detection**\n",
        "    * Corner detection: It detects the corners for an image. [cite: 308] (No specific OpenCV function for corner detection directly provided in the text, although masks for corner detection are illustrated). [cite: 402]\n",
        "\n",
        "* **Video capture**\n",
        "    * Code to load and save a digital video:\n",
        "        ```python\n",
        "        video = cv2.VideoCapture('inputVideo.mp4') #Get video data.\n",
        "        while True:\n",
        "          ret, frame = video.read() #Read a frame.\n",
        "          cv2.imshow('frame', frame) #Display a frame.\n",
        "          if(cv2.waitKey(1): #Press any key to stop the frame.\n",
        "            break\n",
        "          video.release() #Save the video file.\n",
        "        ``` [cite: 104, 105, 106]\n",
        "\n",
        "**Topics that will not In Exam this semester:**\n",
        "\n",
        "* Additive vs Subtractive Colour Models [cite: 305, 307, 660, 669]\n",
        "* Opponent Colour Space (OCS) [cite: 322, 687, 719]\n",
        "* Histogram equalization calculation [cite: 213, 214, 472]\n",
        "* DWT [cite: 242, 252, 540, 541]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "AkGfFnfh018d"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}